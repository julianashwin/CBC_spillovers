fedminutes.df$pub_date <- as.Date(fedminutes.df$pub_date, format = "%d/%m/%Y")
fedminutes.df$meet_date <- as.Date(fedminutes.df$meet_date, format = "%d/%m/%Y")
fedminutes.df <- fedminutes.df[order(fedminutes.df$meet_date),]
# First remove the first paragraph of each document
#View(fedminutes.df[which(fedminutes.df$meet_date != dplyr::lag(fedminutes.df$meet_date)),])
# Now do the same for the last of each document
#View(fedminutes.df[which(fedminutes.df$meet_date != dplyr::lead(fedminutes.df$meet_date)),])
fedminutes.df[which((str_detect(fedminutes.df$paragraph,
"Votes against"))), "paragraph"] <- NA
fedminutes.df[which((str_detect(fedminutes.df$paragraph,
"Votes for"))), "paragraph"] <- NA
fedminutes.df[which((str_detect(fedminutes.df$paragraph,
"Voting against"))), "paragraph"] <- NA
fedminutes.df[which((str_detect(fedminutes.df$paragraph,
"Voting for"))), "paragraph"] <- NA
fedminutes.df[which((str_detect(fedminutes.df$paragraph,
"Absent and"))), "paragraph"] <- NA
# Remove all the deleted paragraphs from the dataframe
fedminutes.df <- fedminutes.df[which(!is.na(fedminutes.df$paragraph)),]
plot(table(fedminutes.df$meet_date))
summary(nchar(fedminutes.df$paragraph))
fedminutes.df$nchar <- nchar(fedminutes.df$paragraph)
# Add unique paragraph identifier
unique_id <- paste0("fed_", 1:nrow(fedminutes.df))
fedminutes.df$unique_id <- unique_id
fedminutes.df$central_bank <- "Federal Reserve"
plot(table(bankminutes.df$meet_date))
summary(nchar(bankminutes.df$paragraph))
fedminutes.df$annex <- 0
meeting.df <- unique(fedminutes.df[, c("year", "month", "central_bank", "pub_date", "meet_date")])
meeting.df <- meeting.df[order(meeting.df$meet_date),]
table(duplicated(meeting.df[,c("year", "month")]))
meeting.df$meeting_id <- paste0("F_", 1:nrow(meeting.df))
fedminutes.df <- merge(fedminutes.df, meeting.df, by = c("year", "month", "central_bank",
"pub_date", "meet_date"), all.x = TRUE)
fedminutes.df <- fedminutes.df[order(fedminutes.df$meet_date),]
fedminutes.df <- fedminutes.df[, c("unique_id", "meeting_id", "year", "month", "document",
"central_bank", "pub_date", "meet_date", "annex", "source", "paragraph")]
# Write the clean Federal Reserve minutes to a file
write.csv(fedminutes.df, file = paste0(clean_dir, "CBC/fedminutes_clean.csv"),
fileEncoding = "utf-8", row.names = FALSE)
### Import the ECB statement data
import_filename <- paste0(raw_dir, "CBC/ecbstatements_clean.txt")
ecbminutes_all <- readtext(import_filename, encoding = "utf-8")
ecbminutes_all <- ecbminutes_all$text
# replace all \n\n(i) type breaks so that they are included in the same paragraph
temp <- as.data.frame(str_locate_all(ecbminutes_all, "\n\n\\("))
ecbminutes_all <- str_replace_all(ecbminutes_all, "\n\n\\(", " \\(")
# Create dataframe for the paragraphs
variables <- c( "year", "month", "document", "paragraph")
ecbstatements.df <- data.frame(matrix(NA,0,length(variables)))
colnames(ecbstatements.df) <- variables
doc.locs <- as.data.frame(str_locate_all(ecbminutes_all, "\n[0-9]+_[0-9]+_[0-9]+_S.txt"))
doc.num <- length(doc.locs[,1])
# Loop over the paragraphs
for (i in 1:(doc.num-1)){
# The whole document
full.doc <- str_trim(str_sub(ecbminutes_all, doc.locs[i,"start"], (doc.locs[(i+1),"start"]-1)))
# Extract the document name at the top of each document
docid <- str_trim(str_sub(full.doc, 1, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"start"]-1)))
full.doc <- str_trim(str_sub(full.doc, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"end"])))
para.locs <- as.data.frame(str_locate_all(full.doc, "\n\n"))
para.num <- length(para.locs[,1])
# Create empty dataframe to fill
temp.df <- data.frame(matrix(NA,(para.num+1),length(variables)))
colnames(temp.df) <- variables
temp.df$document <- docid
temp.df$year <- as.numeric(str_sub(docid, 1, 4))
temp.df$month <- clean_month(as.numeric(str_sub(docid, 6,7)))
text <- str_trim(str_sub(full.doc, 1, (para.locs[(1),"start"]-1)))
temp.df$paragraph[1] <- text
for(j in 2:(para.num)){
text <- str_trim(str_sub(full.doc, (para.locs[j-1,"end"]), (para.locs[(j),"end"])))
temp.df$paragraph[j] <- text
}
# Extra one at the ed because \n\n has been removed by str_trim
text <- str_trim(str_sub(full.doc, (para.locs[para.num,"end"])))
temp.df$paragraph[(para.num+1)] <- text
ecbstatements.df <- rbind(ecbstatements.df, temp.df)
}
full.doc <- str_trim(str_sub(ecbminutes_all, (doc.locs[(doc.num),"start"])))
docid <- str_trim(str_sub(full.doc, 1, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"start"]-1)))
full.doc <- str_trim(str_sub(full.doc, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"end"])))
para.locs <- as.data.frame(str_locate_all(full.doc, "\n\n"))
para.num <- length(para.locs[,1])
temp.df <- data.frame(matrix(NA,para.num,length(variables)))
colnames(temp.df) <- variables
temp.df$document <- docid
temp.df$year <- as.numeric(str_sub(docid, 1, 4))
temp.df$month <- clean_month(as.numeric(str_sub(docid, 6,7)))
text <- str_trim(str_sub(full.doc, 1, (para.locs[(1),"start"]-1)))
temp.df$paragraph[1] <- text
for(j in 2:(para.num)){
text <- str_trim(str_sub(full.doc, (para.locs[j-1,"end"]), (para.locs[(j),"end"])))
temp.df$paragraph[j] <- text
}
ecbstatements.df <- rbind(ecbstatements.df, temp.df)
ecbstatements.df$source <- ecbstatements.df$document
# Import the meeting dates as a double check for ECB
meeting_dates <- read.csv(file = paste0(raw_dir, "CBC/ECB_meeting_dates.csv"),
encoding = "utf-8", stringsAsFactors = FALSE)
ecbstatements.df <- merge(ecbstatements.df, meeting_dates, by = c("year", "month", "source"), all.x = TRUE)
# Remove a handful of duplicates
ecbstatements.df <- ecbstatements.df[!duplicated(ecbstatements.df[,c("year", "month", "source","paragraph")]),]
table(is.na(ecbstatements.df$pub_date))
table(is.na(ecbstatements.df$meet_date))
# Order by meeting date
ecbstatements.df$pub_date <- as.Date(ecbstatements.df$pub_date, format = "%d/%m/%Y")
ecbstatements.df$meet_date <- as.Date(ecbstatements.df$meet_date, format = "%d/%m/%Y")
ecbstatements.df <- ecbstatements.df[order(ecbstatements.df$meet_date),]
### Remove the preamble and logistical comments
# First remove the first paragraph of each document
#View(ecbstatements.df[which(ecbstatements.df$meet_date != dplyr::lag(ecbstatements.df$meet_date)),])
ecbstatements.df[which(ecbstatements.df$paragraph == "ECB Press conference: Introductory statement"), "paragraph"] <- NA
ecbstatements.df[which(ecbstatements.df$meet_date != dplyr::lag(ecbstatements.df$meet_date)),"paragraph"] <- NA
# Inspect the new first paragraph and remove if it doesn't include "Ladies and gentlemen"
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                         !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"])
ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
!str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"] <- NA
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                              !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"])
ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
!str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen, ")),"paragraph"] <- NA
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                              !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"])
# A few ones that slip through to tie up
ecbstatements.df[which(ecbstatements.df$paragraph == "With a Transcript of the questions and answers"), "paragraph"] <- NA
ecbstatements.df[which(ecbstatements.df$paragraph == "Q&A on TARGET2-Securities"), "paragraph"] <- NA
ecbstatements.df[which(ecbstatements.df$paragraph == "Welcome address by Jens Weidmann, President of the Deutsche Bundesbank"), "paragraph"] <- NA
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                              !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),])
# Now do the same for the last of each document
#View(ecbstatements.df[which(ecbstatements.df$meet_date != dplyr::lead(ecbstatements.df$meet_date)),])
#View(ecbstatements.df[which((ecbstatements.df$meet_date != dplyr::lead(ecbstatements.df$meet_date)) &
#                              (str_detect(ecbstatements.df$paragraph, "questions"))), ])
ecbstatements.df[which((ecbstatements.df$meet_date != dplyr::lead(ecbstatements.df$meet_date)) &
(str_detect(ecbstatements.df$paragraph, "questions")) &
!str_detect(ecbstatements.df$paragraph, "statutory function") &
!str_detect(ecbstatements.df$paragraph, "security features") &
!str_detect(ecbstatements.df$paragraph, "sustainable growth")), "paragraph"]<- NA
# Remove all the deleted paragraphs from the dataframe
ecbstatements.df <- ecbstatements.df[which(!is.na(ecbstatements.df$paragraph)),]
plot(table(ecbstatements.df$meet_date))
summary(nchar(ecbstatements.df$paragraph))
ecbstatements.df$nchar <- nchar(ecbstatements.df$paragraph)
# The huge paragraph here is actually correct (https://www.ecb.europa.eu/press/pressconf/1998/html/is981222.en.html)
# Add unique paragraph identifier
unique_id <- paste0("ecb_", 1:nrow(ecbstatements.df))
ecbstatements.df$unique_id <- unique_id
ecbstatements.df$central_bank <- "European Central Bank"
ecbstatements.df$annex <- 0
meeting.df <- unique(ecbstatements.df[, c("year", "month", "central_bank", "pub_date", "meet_date")])
meeting.df <- meeting.df[order(meeting.df$meet_date),]
View(meeting_dates)
View(meeting.df)
meeting.df$year == meeting_dates$year
table(meeting.df$year)
table(meeting_dates$year)
### Import the ECB statement data
import_filename <- paste0(raw_dir, "CBC/ecbstatements_clean.txt")
ecbminutes_all <- readtext(import_filename, encoding = "utf-8")
ecbminutes_all <- ecbminutes_all$text
# replace all \n\n(i) type breaks so that they are included in the same paragraph
temp <- as.data.frame(str_locate_all(ecbminutes_all, "\n\n\\("))
ecbminutes_all <- str_replace_all(ecbminutes_all, "\n\n\\(", " \\(")
# Create dataframe for the paragraphs
variables <- c( "year", "month", "document", "paragraph")
ecbstatements.df <- data.frame(matrix(NA,0,length(variables)))
colnames(ecbstatements.df) <- variables
doc.locs <- as.data.frame(str_locate_all(ecbminutes_all, "\n[0-9]+_[0-9]+_[0-9]+_S.txt"))
doc.num <- length(doc.locs[,1])
# Loop over the paragraphs
for (i in 1:(doc.num-1)){
# The whole document
full.doc <- str_trim(str_sub(ecbminutes_all, doc.locs[i,"start"], (doc.locs[(i+1),"start"]-1)))
# Extract the document name at the top of each document
docid <- str_trim(str_sub(full.doc, 1, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"start"]-1)))
full.doc <- str_trim(str_sub(full.doc, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"end"])))
para.locs <- as.data.frame(str_locate_all(full.doc, "\n\n"))
para.num <- length(para.locs[,1])
# Create empty dataframe to fill
temp.df <- data.frame(matrix(NA,(para.num+1),length(variables)))
colnames(temp.df) <- variables
temp.df$document <- docid
temp.df$year <- as.numeric(str_sub(docid, 1, 4))
temp.df$month <- clean_month(as.numeric(str_sub(docid, 6,7)))
text <- str_trim(str_sub(full.doc, 1, (para.locs[(1),"start"]-1)))
temp.df$paragraph[1] <- text
for(j in 2:(para.num)){
text <- str_trim(str_sub(full.doc, (para.locs[j-1,"end"]), (para.locs[(j),"end"])))
temp.df$paragraph[j] <- text
}
# Extra one at the ed because \n\n has been removed by str_trim
text <- str_trim(str_sub(full.doc, (para.locs[para.num,"end"])))
temp.df$paragraph[(para.num+1)] <- text
ecbstatements.df <- rbind(ecbstatements.df, temp.df)
}
full.doc <- str_trim(str_sub(ecbminutes_all, (doc.locs[(doc.num),"start"])))
docid <- str_trim(str_sub(full.doc, 1, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"start"]-1)))
full.doc <- str_trim(str_sub(full.doc, (as.data.frame(str_locate(full.doc, "\t\t\n\n"))[1,"end"])))
para.locs <- as.data.frame(str_locate_all(full.doc, "\n\n"))
para.num <- length(para.locs[,1])
temp.df <- data.frame(matrix(NA,para.num,length(variables)))
colnames(temp.df) <- variables
temp.df$document <- docid
temp.df$year <- as.numeric(str_sub(docid, 1, 4))
temp.df$month <- clean_month(as.numeric(str_sub(docid, 6,7)))
text <- str_trim(str_sub(full.doc, 1, (para.locs[(1),"start"]-1)))
temp.df$paragraph[1] <- text
for(j in 2:(para.num)){
text <- str_trim(str_sub(full.doc, (para.locs[j-1,"end"]), (para.locs[(j),"end"])))
temp.df$paragraph[j] <- text
}
ecbstatements.df <- rbind(ecbstatements.df, temp.df)
ecbstatements.df$source <- ecbstatements.df$document
# Import the meeting dates as a double check for ECB
meeting_dates <- read.csv(file = paste0(raw_dir, "CBC/ECB_meeting_dates.csv"),
encoding = "utf-8", stringsAsFactors = FALSE)
ecbstatements.df <- merge(ecbstatements.df, meeting_dates, by = c("year", "month", "source"), all.x = TRUE)
# Remove a handful of duplicates
ecbstatements.df <- ecbstatements.df[!duplicated(ecbstatements.df[,c("year", "month", "source","paragraph")]),]
table(is.na(ecbstatements.df$pub_date))
table(is.na(ecbstatements.df$meet_date))
# Order by meeting date
ecbstatements.df$pub_date <- as.Date(ecbstatements.df$pub_date, format = "%d/%m/%Y")
ecbstatements.df$meet_date <- as.Date(ecbstatements.df$meet_date, format = "%d/%m/%Y")
ecbstatements.df <- ecbstatements.df[order(ecbstatements.df$meet_date),]
### Remove the preamble and logistical comments
# First remove the first paragraph of each document
#View(ecbstatements.df[which(ecbstatements.df$meet_date != dplyr::lag(ecbstatements.df$meet_date)),])
ecbstatements.df[which(ecbstatements.df$paragraph == "ECB Press conference: Introductory statement"), "paragraph"] <- NA
ecbstatements.df[which(ecbstatements.df$meet_date != dplyr::lag(ecbstatements.df$meet_date)),"paragraph"] <- NA
# Inspect the new first paragraph and remove if it doesn't include "Ladies and gentlemen"
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                         !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"])
ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
!str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"] <- NA
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                              !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"])
ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
!str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen, ")),"paragraph"] <- NA
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                              !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),"paragraph"])
# A few ones that slip through to tie up
ecbstatements.df[which(ecbstatements.df$paragraph == "With a Transcript of the questions and answers"), "paragraph"] <- NA
ecbstatements.df[which(ecbstatements.df$paragraph == "Q&A on TARGET2-Securities"), "paragraph"] <- NA
ecbstatements.df[which(ecbstatements.df$paragraph == "Welcome address by Jens Weidmann, President of the Deutsche Bundesbank"), "paragraph"] <- NA
#View(ecbstatements.df[which(is.na(dplyr::lag(ecbstatements.df$paragraph)) &
#                              !str_detect(ecbstatements.df$paragraph,"Ladies and gentlemen")),])
# Now do the same for the last of each document
#View(ecbstatements.df[which(ecbstatements.df$meet_date != dplyr::lead(ecbstatements.df$meet_date)),])
#View(ecbstatements.df[which((ecbstatements.df$meet_date != dplyr::lead(ecbstatements.df$meet_date)) &
#                              (str_detect(ecbstatements.df$paragraph, "questions"))), ])
ecbstatements.df[which((ecbstatements.df$meet_date != dplyr::lead(ecbstatements.df$meet_date)) &
(str_detect(ecbstatements.df$paragraph, "questions")) &
!str_detect(ecbstatements.df$paragraph, "statutory function") &
!str_detect(ecbstatements.df$paragraph, "security features") &
!str_detect(ecbstatements.df$paragraph, "sustainable growth")), "paragraph"]<- NA
# Remove all the deleted paragraphs from the dataframe
ecbstatements.df <- ecbstatements.df[which(!is.na(ecbstatements.df$paragraph)),]
plot(table(ecbstatements.df$meet_date))
summary(nchar(ecbstatements.df$paragraph))
ecbstatements.df$nchar <- nchar(ecbstatements.df$paragraph)
# The huge paragraph here is actually correct (https://www.ecb.europa.eu/press/pressconf/1998/html/is981222.en.html)
# Add unique paragraph identifier
unique_id <- paste0("ecb_", 1:nrow(ecbstatements.df))
ecbstatements.df$unique_id <- unique_id
ecbstatements.df$central_bank <- "European Central Bank"
ecbstatements.df$annex <- 0
# Add a unique meeting identifier
meeting.df <- unique(ecbstatements.df[, c("year", "month", "central_bank", "pub_date", "meet_date")])
meeting.df <- meeting.df[order(meeting.df$meet_date),]
table(meeting_dates$year)
table(meeting.df$year)
table(duplicated(meeting.df[,c("year", "month")]))
meeting.df$meeting_id <- paste0("E_", 1:nrow(meeting.df))
ecbstatements.df <- merge(ecbstatements.df, meeting.df, by = c("year", "month", "central_bank",
"pub_date", "meet_date"), all.x = TRUE)
ecbstatements.df <- ecbstatements.df[order(ecbstatements.df$meet_date),]
colnames(bankminutes.df)
ecbstatements.df <- ecbstatements.df[, c("unique_id", "meeting_id", "year", "month", "document",
"central_bank", "pub_date", "meet_date", "annex", "source", "paragraph")]
# Write the clean ECB minutes to a file
write.csv(ecbstatements.df, file = paste0(clean_dir, "CBC/ecbstatements_clean.csv"),
fileEncoding = "utf-8", row.names = FALSE)
setwd("~/Documents/GitHub/CBC_spillovers")
rm(list=ls())
require(tm)
require(stringr)
require(tidytext)
require(tidyr)
require(slam)
### Define the directories where raw data is stored and clean will be saved
clean_dir <- "~/Documents/DPhil/Clean_Data/"
raw_dir <- "~/Documents/DPhil/Raw_Data/"
############################# Import and extract vocab of each corpus #############################
### Import the Federal Reserve data
fedminutes.df <- read.csv(paste0(clean_dir, "CBC/fedminutes_clean.csv"), encoding = "utf-8", stringsAsFactors = FALSE)
# The removePunctuation function seems to be unreliable, so use gsub to remove some problems first
fedminutes.df$paragraph <- gsub("-", " ", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub("/'", "", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub("’", "", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub("‘", "", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub("“", "", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub("”", "", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub("€", " ", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub(",", " ", fedminutes.df$paragraph)
fedminutes.df$paragraph <- gsub(":", " ", fedminutes.df$paragraph)
?stopwords
?stemDocument
inspect(fedminutes.corpus[[4713]])
fedminutes.corpus <- Corpus(VectorSource(unlist(fedminutes.df[, "paragraph"])))
# Preliminary cleaning
inspect(fedminutes.corpus[[4713]])
inspect(fedminutes.corpus[[47]])
fedminutes.corpus <- Corpus(VectorSource(unlist(fedminutes.df[, "paragraph"])))
inspect(fedminutes.corpus[[47]])
inspect(fedminutes.corpus[[473]])
inspect(fedminutes.corpus[[4730]])
inspect(fedminutes.corpus[[43]])
inspect(fedminutes.corpus[[1]])
inspect(fedminutes.corpus[[100]])
inspect(fedminutes.corpus[[200]])
inspect(fedminutes.corpus[[250]])
fedminutes.df[250,]
fedminutes.corpus <- tm_map(fedminutes.corpus, removeNumbers)
inspect(fedminutes.corpus[[250]])
fedminutes.corpus <- tm_map(fedminutes.corpus, removePunctuation)
inspect(fedminutes.corpus[[250]])
fedminutes.corpus <- tm_map(fedminutes.corpus, content_transformer(tolower))
inspect(fedminutes.corpus[[250]])
fedminutes.corpus <- tm_map(fedminutes.corpus, removeWords, stopwords("english"))
inspect(fedminutes.corpus[[250]])
fedminutes.corpus <-  tm_map(fedminutes.corpus, stemDocument)
inspect(fedminutes.corpus[[250]])
fedminutes.dtm <- DocumentTermMatrix(fedminutes.corpus, control = list(minWordLength = 3))
print(paste("Dimensions of fedminutes.dtm are", dim(fedminutes.dtm)[1], "documents and",
dim(fedminutes.dtm)[2], "words in vocab"))
fedminutes.vocab <- fedminutes.dtm$dimnames$Terms
fedminutes.df$wordcount <- rowSums(as.matrix(dtm))
fedminutes.df$wordcount <- rowSums(as.matrix(fedminutes.dtm))
summary(fedminutes.dtm$wordcount)
View(fedminutes.df)
max(fedminutes.dtm$wordcount)
table(is.na(fedminutes.df$wordcount))
sum((fedminutes.df$wordcount))
summary((fedminutes.df$wordcount))
summary(fedminutes.df$wordcount)
sum(fedminutes.dtm$wordcount)
sum(fedminutes.df$wordcount)
length(unique(fedminutes.df$meeting_id))
### Import the Bank of England data
bankminutes.df <- read.csv(paste0(clean_dir, "CBC/bankminutes_clean.csv"), encoding = "utf-8", stringsAsFactors = FALSE)
bankminutes.df$paragraph <- gsub("-", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("/'", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("’", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("‘", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("“", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("”", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("€", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub(",", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub(":", " ", bankminutes.df$paragraph)
bankminutes.corpus <- Corpus(VectorSource(unlist(bankminutes.df[, "paragraph"])))
# Preliminary cleaning
bankminutes.corpus <- tm_map(bankminutes.corpus, stripWhitespace)
bankminutes.corpus <- tm_map(bankminutes.corpus, removeNumbers)
bankminutes.corpus <- tm_map(bankminutes.corpus, removePunctuation)
bankminutes.corpus <- tm_map(bankminutes.corpus, content_transformer(tolower))
bankminutes.corpus <- tm_map(bankminutes.corpus, removeWords, stopwords("english"))
bankminutes.corpus <- tm_map(bankminutes.corpus, stemDocument)
# The convert each corpus to a DTM in order to extract the complete vocab
bankminutes.dtm <- DocumentTermMatrix(bankminutes.corpus, control = list(minWordLength = 3))
print(paste("Dimensions of bankminutes.dtm are", dim(bankminutes.dtm)[1], "documents and",
dim(bankminutes.dtm)[2], "words in vocab"))
bankminutes.vocab <- bankminutes.dtm$dimnames$Terms
bankminutes.df$wordcount <- rowSums(as.matrix(bankminutes.dtm))
summary(bankminutes.df$wordcount)
sum(bankminutes.df$wordcount)
length(unique(bankminutes.df$meeting_id))
exclude_annex <- TRUE
if (exclude_annex){
bankminutes.df <- bankminutes.df[which(bankminutes.df$annex == 0),]
}
bankminutes.df <- read.csv(paste0(clean_dir, "CBC/bankminutes_clean.csv"), encoding = "utf-8", stringsAsFactors = FALSE)
bankminutes.df$paragraph <- gsub("-", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("/'", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("’", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("‘", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("“", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("”", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("€", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub(",", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub(":", " ", bankminutes.df$paragraph)
bankminutes.corpus <- Corpus(VectorSource(unlist(bankminutes.df[, "paragraph"])))
# Preliminary cleaning
bankminutes.corpus <- tm_map(bankminutes.corpus, stripWhitespace)
bankminutes.corpus <- tm_map(bankminutes.corpus, removeNumbers)
bankminutes.corpus <- tm_map(bankminutes.corpus, removePunctuation)
bankminutes.corpus <- tm_map(bankminutes.corpus, content_transformer(tolower))
bankminutes.corpus <- tm_map(bankminutes.corpus, removeWords, stopwords("english"))
bankminutes.corpus <- tm_map(bankminutes.corpus, stemDocument)
# The convert each corpus to a DTM in order to extract the complete vocab
bankminutes.dtm <- DocumentTermMatrix(bankminutes.corpus, control = list(minWordLength = 3))
print(paste("Dimensions of bankminutes.dtm are", dim(bankminutes.dtm)[1], "documents and",
dim(bankminutes.dtm)[2], "words in vocab"))
bankminutes.vocab <- bankminutes.dtm$dimnames$Terms
bankminutes.df <- read.csv(paste0(clean_dir, "CBC/bankminutes_clean.csv"), encoding = "utf-8", stringsAsFactors = FALSE)
# Toggle the "exclude annex" option
exclude_annex <- TRUE
if (exclude_annex){
bankminutes.df <- bankminutes.df[which(bankminutes.df$annex == 0),]
}
bankminutes.df$paragraph <- gsub("-", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("/'", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("’", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("‘", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("“", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("”", "", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub("€", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub(",", " ", bankminutes.df$paragraph)
bankminutes.df$paragraph <- gsub(":", " ", bankminutes.df$paragraph)
bankminutes.corpus <- Corpus(VectorSource(unlist(bankminutes.df[, "paragraph"])))
# Preliminary cleaning
bankminutes.corpus <- tm_map(bankminutes.corpus, stripWhitespace)
bankminutes.corpus <- tm_map(bankminutes.corpus, removeNumbers)
bankminutes.corpus <- tm_map(bankminutes.corpus, removePunctuation)
bankminutes.corpus <- tm_map(bankminutes.corpus, content_transformer(tolower))
bankminutes.corpus <- tm_map(bankminutes.corpus, removeWords, stopwords("english"))
bankminutes.corpus <- tm_map(bankminutes.corpus, stemDocument)
# The convert each corpus to a DTM in order to extract the complete vocab
bankminutes.dtm <- DocumentTermMatrix(bankminutes.corpus, control = list(minWordLength = 3))
print(paste("Dimensions of bankminutes.dtm are", dim(bankminutes.dtm)[1], "documents and",
dim(bankminutes.dtm)[2], "words in vocab"))
bankminutes.vocab <- bankminutes.dtm$dimnames$Terms
bankminutes.df$wordcount <- rowSums(as.matrix(bankminutes.dtm))
summary(bankminutes.df$wordcount)
sum(bankminutes.df$wordcount)
length(unique(bankminutes.df$meeting_id))
ecbstatements.df <- read.csv(paste0(clean_dir, "CBC/ecbstatements_clean.csv"), encoding = "utf-8", stringsAsFactors = FALSE)
ecbstatements.df$paragraph <- gsub("-", " ", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub("/'", "", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub("’", "", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub("‘", "", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub("“", "", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub("”", "", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub("€", " ", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub(",", " ", ecbstatements.df$paragraph)
ecbstatements.df$paragraph <- gsub(":", " ", ecbstatements.df$paragraph)
ecbstatements.corpus <- Corpus(VectorSource(unlist(ecbstatements.df[, "paragraph"])))
# Preliminary cleaning
ecbstatements.corpus <- tm_map(ecbstatements.corpus, stripWhitespace)
ecbstatements.corpus <- tm_map(ecbstatements.corpus, removeNumbers)
ecbstatements.corpus <- tm_map(ecbstatements.corpus, removePunctuation)
ecbstatements.corpus <- tm_map(ecbstatements.corpus, content_transformer(tolower))
ecbstatements.corpus <- tm_map(ecbstatements.corpus, removeWords, stopwords("english"))
ecbstatements.corpus <- tm_map(ecbstatements.corpus, stemDocument)
ecbstatements.dtm <- DocumentTermMatrix(ecbstatements.corpus, control = list(minWordLength = 3))
print(paste("Dimensions of ecbstatements.dtm are", dim(ecbstatements.dtm)[1], "documents and",
dim(ecbstatements.dtm)[2], "words in vocab"))
ecbstatements.vocab <- ecbstatements.dtm$dimnames$Terms
ecbstatements.df$wordcount <- rowSums(as.matrix(ecbstatements.dtm))
summary(ecbstatements.df$wordcount)
sum(ecbstatements.df$wordcount)
length(unique(ecbstatements.df$meeting_id))
a1 <- c("a","b","c","d","e")
a2 <- c("a","b","c","f","g")
a3 <- c("a","b","c","g","h")
# So all elements that appear in at least two lists should be given by
atleasttwo <- union(union(intersect(a1,a2), intersect(a1,a3)), intersect(a2,a3))
print(atleasttwo)
# And thus elements that do not appear in at least two should be given by
total <- union(union(a1,a2),a3)
print(total)
outersect(total, atleasttwo)
# First a little example to demonstrate the outersect methodology
outersect <- function(x, y) {
sort(c(setdiff(x, y),
setdiff(y, x)))
}
a1 <- c("a","b","c","d","e")
a2 <- c("a","b","c","f","g")
a3 <- c("a","b","c","g","h")
# So all elements that appear in at least two lists should be given by
atleasttwo <- union(union(intersect(a1,a2), intersect(a1,a3)), intersect(a2,a3))
print(atleasttwo)
# And thus elements that do not appear in at least two should be given by
total <- union(union(a1,a2),a3)
print(total)
outersect(total, atleasttwo)
# Now the same with the vocabularies
fedbank.vocab <- intersect(fedminutes.vocab, bankminutes.vocab)
fedecb.vocab <- intersect(fedminutes.vocab, ecbstatements.vocab)
bankecb.vocab <- intersect(bankminutes.vocab, ecbstatements.vocab)
atleasttwo <- union(union(fedbank.vocab, fedecb.vocab), bankecb.vocab)
allterms <- union(union(fedminutes.vocab, bankminutes.vocab), ecbstatements.vocab)
onecorpusterms <- outersect(allterms, atleasttwo)
# Now combine all three corpora into one
alldata.df <- rbind(fedminutes.df, bankminutes.df)
alldata.df <- rbind(alldata.df, ecbstatements.df)
alldata.corpus <- Corpus(VectorSource(unlist(alldata.df[, "paragraph"])))
meta(alldata.corpus, type = "indexed", tag = "unique_id") <- alldata.df$unique_id
meta(alldata.corpus, type = "indexed", tag = "central_bank") <- alldata.df$central_bank
meta(alldata.corpus, type = "indexed", tag = "meet_date") <- alldata.df$meet_date
meta(alldata.corpus, type = "indexed", tag = "pub_date") <- alldata.df$pub_date
alldata.corpus <- tm_map(alldata.corpus, stripWhitespace)
alldata.corpus <- tm_map(alldata.corpus, removeNumbers)
alldata.corpus <- tm_map(alldata.corpus, removePunctuation)
alldata.corpus <- tm_map(alldata.corpus, content_transformer(tolower))
alldata.corpus <- tm_map(alldata.corpus, removeWords, stopwords("english"))
alldata.corpus <- tm_map(alldata.corpus, stemDocument)
alldata.corpus <- tm_map(alldata.corpus, removeWords, onecorpusterms)
